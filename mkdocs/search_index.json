{
    "docs": [
        {
            "location": "/", 
            "text": "OnlineStats.jl\n\n\nOnlineStats\n is a Julia package which provides online algorithms for statistical models.\n\n\nOnline algorithms are well suited for streaming data or when data is too large to hold in memory.\n\n\nObservations are processed one at a time and all \nalgorithms use O(1) memory\n.\n\n\n\n\nOverview\n\n\nEvery OnlineStat is a Type\n\n\nusing\n \nOnlineStats\n\n\no\n \n=\n \nMean\n()\n\n\n\n\n\n\nAll OnlineStats can be updated\n\n\ny\n \n=\n \nrandn\n(\n100\n)\n\n\n\nfor\n \nyi\n \nin\n \ny\n\n    \nfit!\n(\no\n,\n \ny\n)\n\n\nend\n\n\n\n# or more simply:\n\n\nfit!\n(\no\n,\n \ny\n)\n\n\n\n\n\n\nOnlineStats share a common interface\n\n\nvalue\n(\no\n)\n  \n# associated value of an OnlineStat\n\n\nnobs\n(\no\n)\n   \n# number of observations used\n\n\n\n\n\n\n\n\nWhat Can OnlineStats Do?\n\n\nWhile many estimates can be calculated analytically with an online algorithm, several\ntype rely on stochastic approximation.\n\n\nSummary Statistics\n\n\n\n\nMean: \nMean\n, \nMeans\n\n\nVariance: \nVariance\n, \nVariances\n\n\nQuantiles: \nQuantileMM\n, \nQuantileSGD\n\n\nCovariance Matrix: \nCovMatrix\n\n\nMaximum and Minimum:  \nExtrema\n\n\nSkewness and Kurtosis:  \nMoments\n\n\nSum/Differences:  \nSum\n, \nSums\n, \nDiff\n, \nDiffs\n\n\n\n\nDensity Estimation\n\n\n\n\ndistributionfit(D, data)\n\n\nFor \nD in [Beta, Categorical, Cauchy, Gamma, LogNormal, Normal, Multinomial, MvNormal]\n\n\n\n\n\n\nGaussian Mixtures: \nNormalMix\n\n\n\n\nPredictive Modeling\n\n\n\n\nLinear Regression: \nLinReg\n, \nStatLearn\n\n\nLogistic Regression: \nStatLearn\n\n\nPoisson Regression: \nStatLearn\n\n\nSupport Vector Machines: \nStatLearn\n\n\nQuantile Regression: \nStatLearn\n, \nQuantRegMM\n\n\nHuber Loss Regression: \nStatLearn\n\n\nL1 Loss Regression: \nStatLearn\n\n\n\n\nOther\n\n\n\n\nK-Means clustering: \nKMeans\n\n\nBootstrapping: \nBernoulliBootstrap\n, \nPoissonBootstrap\n\n\nApproximate count of distinct elements: \nHyperLogLog", 
            "title": "Home"
        }, 
        {
            "location": "/#onlinestatsjl", 
            "text": "OnlineStats  is a Julia package which provides online algorithms for statistical models.  Online algorithms are well suited for streaming data or when data is too large to hold in memory.  Observations are processed one at a time and all  algorithms use O(1) memory .", 
            "title": "OnlineStats.jl"
        }, 
        {
            "location": "/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/#every-onlinestat-is-a-type", 
            "text": "using   OnlineStats  o   =   Mean ()", 
            "title": "Every OnlineStat is a Type"
        }, 
        {
            "location": "/#all-onlinestats-can-be-updated", 
            "text": "y   =   randn ( 100 )  for   yi   in   y \n     fit! ( o ,   y )  end  # or more simply:  fit! ( o ,   y )", 
            "title": "All OnlineStats can be updated"
        }, 
        {
            "location": "/#onlinestats-share-a-common-interface", 
            "text": "value ( o )    # associated value of an OnlineStat  nobs ( o )     # number of observations used", 
            "title": "OnlineStats share a common interface"
        }, 
        {
            "location": "/#what-can-onlinestats-do", 
            "text": "While many estimates can be calculated analytically with an online algorithm, several\ntype rely on stochastic approximation.", 
            "title": "What Can OnlineStats Do?"
        }, 
        {
            "location": "/#summary-statistics", 
            "text": "Mean:  Mean ,  Means  Variance:  Variance ,  Variances  Quantiles:  QuantileMM ,  QuantileSGD  Covariance Matrix:  CovMatrix  Maximum and Minimum:   Extrema  Skewness and Kurtosis:   Moments  Sum/Differences:   Sum ,  Sums ,  Diff ,  Diffs", 
            "title": "Summary Statistics"
        }, 
        {
            "location": "/#density-estimation", 
            "text": "distributionfit(D, data)  For  D in [Beta, Categorical, Cauchy, Gamma, LogNormal, Normal, Multinomial, MvNormal]    Gaussian Mixtures:  NormalMix", 
            "title": "Density Estimation"
        }, 
        {
            "location": "/#predictive-modeling", 
            "text": "Linear Regression:  LinReg ,  StatLearn  Logistic Regression:  StatLearn  Poisson Regression:  StatLearn  Support Vector Machines:  StatLearn  Quantile Regression:  StatLearn ,  QuantRegMM  Huber Loss Regression:  StatLearn  L1 Loss Regression:  StatLearn", 
            "title": "Predictive Modeling"
        }, 
        {
            "location": "/#other", 
            "text": "K-Means clustering:  KMeans  Bootstrapping:  BernoulliBootstrap ,  PoissonBootstrap  Approximate count of distinct elements:  HyperLogLog", 
            "title": "Other"
        }, 
        {
            "location": "/weight/", 
            "text": "Weighting\n\n\nOnlineStats are parameterized by a \nWeight\n type that determines the influence of the\nnext observation.  The \nWeight\n is always the last argument of the constructor.\n\n\n\n\nWeight Types\n\n\nMany updates take the form of a weighted average.  For a current estimate \n\\theta^{(t-1)}\n and new value \nx_t\n, we update the estimate with:\n\n\n\n\n\\theta^{(t)} = (1 - \\gamma_t) \\theta^{(t-1)} + \\gamma_t \\; x_t\n\n\n\n\nConsider how the weights \n\\gamma_t\n affect the influence of the new value on the estimate.  The weight types below will be explained in terms of the weights \n\\gamma_t\n.\n\n\nEqualWeight\n\n\nEqualWeight()\n\n\n\n\n\nMany online algorithms produce the same results as offline counterparts when using \nEqualWeight\n.  Each observation contributes equally.  This is the most common default.\n\n\n\n\n\\gamma_t = \\frac{1}{t}\n\n\n\n\nExponentialWeight\n\n\nExponentialWeight(\u03bb::Float64)\nExponentialWeight(lookback::Int)\n\n\n\n\n\nThe update weight is constant, so newer observations have higher influence.\n\n\n\n\n\\gamma_t = \\lambda\n\n\n\n\nwhere \n\u03bb = 2 / (lookback + 1)\n\n\nBoundedEqualWeight\n\n\nBoundedEqualWeight(\u03bb::Float64)\nBoundedEqualWeight(lookback::Int)\n\n\n\n\n\nUse \nEqualWeight\n until a minimum weight is reached, then uses \nExponentialWeight\n\n\n\n\n\\gamma_t = \\text{max}\\left(\\lambda, \\frac{1}{t}\\right)\n\n\n\n\nLearningRate\n\n\nLearningRate(r::Float64 = .5, \u03bb::Float64 = 0.0)\n\n\n\n\n\nMainly for algorithms using stochastic approximation.  The weights decrease at a \"slow\" rate\nuntil reaching a minimum weight, then uses \nExponentialWeight\n.\n\n\n\n\n\\gamma_t = \\text{max}\\left(\\lambda, \\frac{1}{t^r}\\right), \\quad r \\in [.5, 1]\n\n\n\n\nOverride the Weight\n\n\nYou can override an OnlineStat's Weight with an additional argument to \nfit!\n.  \n\n\ny\n \n=\n \nrandn\n(\n1000\n)\n\n\no\n \n=\n \nMean\n(\nEqualWeight\n())\n\n\nfit!\n(\no\n,\n \ny\n,\n \n.\n01\n)\n  \n# use weight of .01 for each observation\n\n\n\nwts\n \n=\n \nrand\n(\n1000\n)\n\n\nfit!\n(\no\n,\n \ny\n,\n \nwts\n)\n  \n# use weight of wts[i] for observation y[i]", 
            "title": "Weighting"
        }, 
        {
            "location": "/weight/#weighting", 
            "text": "OnlineStats are parameterized by a  Weight  type that determines the influence of the\nnext observation.  The  Weight  is always the last argument of the constructor.", 
            "title": "Weighting"
        }, 
        {
            "location": "/weight/#weight-types", 
            "text": "Many updates take the form of a weighted average.  For a current estimate  \\theta^{(t-1)}  and new value  x_t , we update the estimate with:   \\theta^{(t)} = (1 - \\gamma_t) \\theta^{(t-1)} + \\gamma_t \\; x_t   Consider how the weights  \\gamma_t  affect the influence of the new value on the estimate.  The weight types below will be explained in terms of the weights  \\gamma_t .", 
            "title": "Weight Types"
        }, 
        {
            "location": "/weight/#equalweight", 
            "text": "EqualWeight()  Many online algorithms produce the same results as offline counterparts when using  EqualWeight .  Each observation contributes equally.  This is the most common default.   \\gamma_t = \\frac{1}{t}", 
            "title": "EqualWeight"
        }, 
        {
            "location": "/weight/#exponentialweight", 
            "text": "ExponentialWeight(\u03bb::Float64)\nExponentialWeight(lookback::Int)  The update weight is constant, so newer observations have higher influence.   \\gamma_t = \\lambda   where  \u03bb = 2 / (lookback + 1)", 
            "title": "ExponentialWeight"
        }, 
        {
            "location": "/weight/#boundedequalweight", 
            "text": "BoundedEqualWeight(\u03bb::Float64)\nBoundedEqualWeight(lookback::Int)  Use  EqualWeight  until a minimum weight is reached, then uses  ExponentialWeight   \\gamma_t = \\text{max}\\left(\\lambda, \\frac{1}{t}\\right)", 
            "title": "BoundedEqualWeight"
        }, 
        {
            "location": "/weight/#learningrate", 
            "text": "LearningRate(r::Float64 = .5, \u03bb::Float64 = 0.0)  Mainly for algorithms using stochastic approximation.  The weights decrease at a \"slow\" rate\nuntil reaching a minimum weight, then uses  ExponentialWeight .   \\gamma_t = \\text{max}\\left(\\lambda, \\frac{1}{t^r}\\right), \\quad r \\in [.5, 1]", 
            "title": "LearningRate"
        }, 
        {
            "location": "/weight/#override-the-weight", 
            "text": "You can override an OnlineStat's Weight with an additional argument to  fit! .    y   =   randn ( 1000 )  o   =   Mean ( EqualWeight ())  fit! ( o ,   y ,   . 01 )    # use weight of .01 for each observation  wts   =   rand ( 1000 )  fit! ( o ,   y ,   wts )    # use weight of wts[i] for observation y[i]", 
            "title": "Override the Weight"
        }, 
        {
            "location": "/callback/", 
            "text": "Callbacks\n\n\nWhile an OnlineStat is being updated, you may wish to perform an action like print intermediate results to a log file or update a plot.  For this purpose, OnlineStats exports a \nmaprows\n function.\n\n\nmaprows(f::Function, b::Integer, data...)\n\n\nmaprows\n works similar to \nBase.mapslices\n, but maps \nb\n rows at a time.  It is best used with Julia's do block syntax.\n\n\nExample 1\n\n\nInput\n\n\ny\n \n=\n \nrandn\n(\n100\n)\n\n\no\n \n=\n \nMean\n()\n\n\nmaprows\n(\n20\n,\n \ny\n)\n \ndo\n \nyi\n\n    \nfit!\n(\no\n,\n \nyi\n)\n\n    \ninfo\n(\nvalue of mean is \n$(mean(o))\n)\n\n\nend\n\n\n\n\n\n\nOutput\n\n\nINFO\n:\n \nvalue\n \nof\n \nmean\n \nis\n \n0.06340121912925167\n\n\nINFO\n:\n \nvalue\n \nof\n \nmean\n \nis\n \n-\n0.06576995293439102\n\n\nINFO\n:\n \nvalue\n \nof\n \nmean\n \nis\n \n0.05374292238752276\n\n\nINFO\n:\n \nvalue\n \nof\n \nmean\n \nis\n \n0.008857939006120167\n\n\nINFO\n:\n \nvalue\n \nof\n \nmean\n \nis\n \n0.016199508928045905\n\n\n\n\n\n\nExample 2\n\n\nInput\n\n\nusing\n \nPlots\n;\n \npyplot\n()\n\n\ny\n \n=\n \nrandn\n(\n10_000\n)\n\n\n\no\n \n=\n \nQuantileMM\n(\nLearningRate\n(\n.\n7\n),\n \ntau\n \n=\n \n[\n.\n25\n,\n \n.\n5\n,\n \n.\n75\n])\n\n\n\nplt\n \n=\n \nplot\n(\nzeros\n(\n1\n,\n \n3\n),\n \nzeros\n(\n1\n,\n \n3\n))\n       \n# initialize plot\n\n\n\nmaprows\n(\n50\n,\n \ny\n)\n \ndo\n \nyi\n              \n# for each batch of 50 observations\n\n    \nfit!\n(\no\n,\n \nyi\n,\n \n5\n)\n                 \n# fit in minibatches of 5\n\n    \npush!\n(\nplt\n,\n \nnobs\n(\no\n),\n \nvalue\n(\no\n))\n  \n# Add a value to the plot\n\n\nend\n\n\n\ndisplay\n(\nplt\n)\n\n\n\n\n\n\nOutput\n\n\n\n\nExample 3\n\n\nInput\n\n\nx\n \n=\n \nrandn\n(\n1000\n,\n \n5\n)\n\n\n\u03b2\n \n=\n \ncollect\n(\n1.\n:\n5\n)\n\n\ny\n \n=\n \nx\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\n1000\n)\n\n\n\no\n \n=\n \nLinReg\n(\n5\n)\n\n\n\nmaprows\n(\n100\n,\n \nx\n,\n \ny\n)\n \ndo\n \nxi\n,\n \nyi\n\n    \nfit!\n(\no\n,\n \nxi\n,\n \nyi\n)\n\n    \ninfo\n(\nYou have fit \n$(nobs(o))\n observations\n)\n\n\nend\n\n\n\n\n\n\nOutput\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n100\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n200\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n300\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n400\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n500\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n600\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n700\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n800\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n900\n \nobservations\n\n\nINFO\n:\n \nYou\n \nhave\n \nfit\n \n1000\n \nobservations", 
            "title": "Callbacks"
        }, 
        {
            "location": "/callback/#callbacks", 
            "text": "While an OnlineStat is being updated, you may wish to perform an action like print intermediate results to a log file or update a plot.  For this purpose, OnlineStats exports a  maprows  function.  maprows(f::Function, b::Integer, data...)  maprows  works similar to  Base.mapslices , but maps  b  rows at a time.  It is best used with Julia's do block syntax.", 
            "title": "Callbacks"
        }, 
        {
            "location": "/callback/#example-1", 
            "text": "", 
            "title": "Example 1"
        }, 
        {
            "location": "/callback/#input", 
            "text": "y   =   randn ( 100 )  o   =   Mean ()  maprows ( 20 ,   y )   do   yi \n     fit! ( o ,   yi ) \n     info ( value of mean is  $(mean(o)) )  end", 
            "title": "Input"
        }, 
        {
            "location": "/callback/#output", 
            "text": "INFO :   value   of   mean   is   0.06340121912925167  INFO :   value   of   mean   is   - 0.06576995293439102  INFO :   value   of   mean   is   0.05374292238752276  INFO :   value   of   mean   is   0.008857939006120167  INFO :   value   of   mean   is   0.016199508928045905", 
            "title": "Output"
        }, 
        {
            "location": "/callback/#example-2", 
            "text": "", 
            "title": "Example 2"
        }, 
        {
            "location": "/callback/#input_1", 
            "text": "using   Plots ;   pyplot ()  y   =   randn ( 10_000 )  o   =   QuantileMM ( LearningRate ( . 7 ),   tau   =   [ . 25 ,   . 5 ,   . 75 ])  plt   =   plot ( zeros ( 1 ,   3 ),   zeros ( 1 ,   3 ))         # initialize plot  maprows ( 50 ,   y )   do   yi                # for each batch of 50 observations \n     fit! ( o ,   yi ,   5 )                   # fit in minibatches of 5 \n     push! ( plt ,   nobs ( o ),   value ( o ))    # Add a value to the plot  end  display ( plt )", 
            "title": "Input"
        }, 
        {
            "location": "/callback/#output_1", 
            "text": "", 
            "title": "Output"
        }, 
        {
            "location": "/callback/#example-3", 
            "text": "", 
            "title": "Example 3"
        }, 
        {
            "location": "/callback/#input_2", 
            "text": "x   =   randn ( 1000 ,   5 )  \u03b2   =   collect ( 1. : 5 )  y   =   x   *   \u03b2   +   randn ( 1000 )  o   =   LinReg ( 5 )  maprows ( 100 ,   x ,   y )   do   xi ,   yi \n     fit! ( o ,   xi ,   yi ) \n     info ( You have fit  $(nobs(o))  observations )  end", 
            "title": "Input"
        }, 
        {
            "location": "/callback/#output_2", 
            "text": "INFO :   You   have   fit   100   observations  INFO :   You   have   fit   200   observations  INFO :   You   have   fit   300   observations  INFO :   You   have   fit   400   observations  INFO :   You   have   fit   500   observations  INFO :   You   have   fit   600   observations  INFO :   You   have   fit   700   observations  INFO :   You   have   fit   800   observations  INFO :   You   have   fit   900   observations  INFO :   You   have   fit   1000   observations", 
            "title": "Output"
        }, 
        {
            "location": "/StatLearn/", 
            "text": "StatLearn\n\n\nApproximate solutions to statistical learning problems using online algorithms.  \nStatLearn\n has extremely fast fitting times.  Number of operations per update is linear with respect to the number of parameters.\n\n\nStatLearn\n provides multiple algorithms for problems of the form\n\n\n\n\n\\frac{1}{T}\\sum_{t=1}^T f_t(\\beta) + \\lambda \\; g(\\beta),\n\n\n\n\nwhere \nf_t\n is the loss at time/update \nt\n, \ng\n is a penalty/regularization term, and \n\\lambda\n is the regularization parameter.\n\n\nStatLearn is parameterized by three main types\n\n\n\n\nNote\n\n\nThe idea is to use \nAlgorithm\n to solve problems of the form \nModelDefinition + Penalty\n.\n\n\n\n\nModelDefinition\n\n\n\n\nL2Regression()\n\n\nSquared error loss.  Default.\n\n\n\n\n\n\nL1Regression()\n\n\nAbsolute loss\n\n\n\n\n\n\nLogisticRegression()\n\n\nModel for data in {0, 1}\n\n\n\n\n\n\nPoissonRegression()\n\n\nModel count data {0, 1, 2, 3, ...}\n\n\n\n\n\n\nQuantileRegression(\u03c4)\n\n\nModel conditional quantiles\n\n\n\n\n\n\nSVMLike()\n\n\nFor data in {-1, 1}.  With \nNoPenalty\n, this is a perceptron.  With \nRidgePenalty\n, this is a support vector machine.\n\n\n\n\n\n\nHuberRegression(\u03b4)\n\n\nRobust Huber loss\n\n\n\n\n\n\n\n\nPenalty\n\n\n\n\nNoPenalty()\n\n\nNo penalty.  Default.\n\n\n\n\n\n\nRidgePenalty(\u03bb)\n\n\nRidge regularization\n\n\n\n\n\n\nLassoPenalty(\u03bb)\n\n\nLASSO regularization\n\n\n\n\n\n\nElasticNetPenalty(\u03bb, \u03b1)\n\n\nWeighted average of Ridge and LASSO.  \n\u03b1 = 0\n is Ridge, \n\u03b1 = 1\n is LASSO.\n\n\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nSGD()\n\n\nStochastic gradient descent.  Default.\n\n\n\n\n\n\nMomentum(\u03b1 = .1)\n\n\nSGD with momentum.\n\n\n\n\n\n\nAdaGrad()\n\n\nAdaptive gradient method. Ignores \nWeight\n.\n\n\n\n\n\n\nAdaDelta(\u03c1 = .001)\n\n\nEssentially AdaGrad with momentum and altered Hessian approximation.  Ignores \nWeight\n.\n\n\n\n\n\n\nADAM(m1 = .01, m2 = .01)\n\n\nm1\n is \"momentum\" for first moment of update\n\n\nm2\n is \"momentum\" for second moment\n\n\n\n\n\n\n\n\nLearning rates and batch sizes matter\n\n\nUsing mini-batches, gradient estimates are less noisy.  The trade-off,\nof course, is that fewer updates occur.\n\n\no1\n \n=\n \nStatLearn\n(\nx\n,\n \ny\n,\n \nSGD\n(),\n \nLearningRate\n(\n.\n6\n))\n      \n# batch size = 1\n\n\no2\n \n=\n \nStatLearn\n(\nx\n,\n \ny\n,\n \n10\n,\n \nLearningRate\n(\n.\n6\n),\n \nSGD\n())\n  \n# batch size = 10\n\n\n\n\n\n\n\n\nNote\n\n\nAny order of \nWeight\n, \nAlgorithm\n, \nModelDefinition\n, and \nPenalty\n arguments are\naccepted by \nStatLearn\n.\n\n\n\n\nExtensions\n\n\nStatLearnCV\n\n\nTODO", 
            "title": "Predictive Modeling"
        }, 
        {
            "location": "/StatLearn/#statlearn", 
            "text": "Approximate solutions to statistical learning problems using online algorithms.   StatLearn  has extremely fast fitting times.  Number of operations per update is linear with respect to the number of parameters.  StatLearn  provides multiple algorithms for problems of the form   \\frac{1}{T}\\sum_{t=1}^T f_t(\\beta) + \\lambda \\; g(\\beta),   where  f_t  is the loss at time/update  t ,  g  is a penalty/regularization term, and  \\lambda  is the regularization parameter.", 
            "title": "StatLearn"
        }, 
        {
            "location": "/StatLearn/#statlearn-is-parameterized-by-three-main-types", 
            "text": "Note  The idea is to use  Algorithm  to solve problems of the form  ModelDefinition + Penalty .", 
            "title": "StatLearn is parameterized by three main types"
        }, 
        {
            "location": "/StatLearn/#modeldefinition", 
            "text": "L2Regression()  Squared error loss.  Default.    L1Regression()  Absolute loss    LogisticRegression()  Model for data in {0, 1}    PoissonRegression()  Model count data {0, 1, 2, 3, ...}    QuantileRegression(\u03c4)  Model conditional quantiles    SVMLike()  For data in {-1, 1}.  With  NoPenalty , this is a perceptron.  With  RidgePenalty , this is a support vector machine.    HuberRegression(\u03b4)  Robust Huber loss", 
            "title": "ModelDefinition"
        }, 
        {
            "location": "/StatLearn/#penalty", 
            "text": "NoPenalty()  No penalty.  Default.    RidgePenalty(\u03bb)  Ridge regularization    LassoPenalty(\u03bb)  LASSO regularization    ElasticNetPenalty(\u03bb, \u03b1)  Weighted average of Ridge and LASSO.   \u03b1 = 0  is Ridge,  \u03b1 = 1  is LASSO.", 
            "title": "Penalty"
        }, 
        {
            "location": "/StatLearn/#algorithm", 
            "text": "SGD()  Stochastic gradient descent.  Default.    Momentum(\u03b1 = .1)  SGD with momentum.    AdaGrad()  Adaptive gradient method. Ignores  Weight .    AdaDelta(\u03c1 = .001)  Essentially AdaGrad with momentum and altered Hessian approximation.  Ignores  Weight .    ADAM(m1 = .01, m2 = .01)  m1  is \"momentum\" for first moment of update  m2  is \"momentum\" for second moment", 
            "title": "Algorithm"
        }, 
        {
            "location": "/StatLearn/#learning-rates-and-batch-sizes-matter", 
            "text": "Using mini-batches, gradient estimates are less noisy.  The trade-off,\nof course, is that fewer updates occur.  o1   =   StatLearn ( x ,   y ,   SGD (),   LearningRate ( . 6 ))        # batch size = 1  o2   =   StatLearn ( x ,   y ,   10 ,   LearningRate ( . 6 ),   SGD ())    # batch size = 10    Note  Any order of  Weight ,  Algorithm ,  ModelDefinition , and  Penalty  arguments are\naccepted by  StatLearn .", 
            "title": "Learning rates and batch sizes matter"
        }, 
        {
            "location": "/StatLearn/#extensions", 
            "text": "", 
            "title": "Extensions"
        }, 
        {
            "location": "/StatLearn/#statlearncv", 
            "text": "TODO", 
            "title": "StatLearnCV"
        }, 
        {
            "location": "/api/", 
            "text": "API\n\n\nADAM\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.ADAM \n: OnlineStats.Algorithm\n\n\n\n\n\nFields:\n\n\nm1 :: Float64\nm2 :: Float64\n\n\n\n\n\nAdaDelta\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.AdaDelta \n: OnlineStats.Algorithm\n\n\n\n\n\nFields:\n\n\n\u03c1 :: Float64\n\n\n\n\n\nAdaGrad\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.AdaGrad \n: OnlineStats.Algorithm\n\n\n\n\n\nAdaGrad2\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.AdaGrad2 \n: OnlineStats.Algorithm\n\n\n\n\n\nAlgorithm\n\n\nNo documentation found.\n\n\nSummary:\n\n\nabstract OnlineStats.Algorithm \n: Any\n\n\n\n\n\nSubtypes:\n\n\nOnlineStats.ADAM\nOnlineStats.AdaDelta\nOnlineStats.AdaGrad\nOnlineStats.AdaGrad2\nOnlineStats.Momentum\nOnlineStats.SGD\n\n\n\n\n\nBernoulliBootstrap\n\n\nBernoulliBootstrap(o::OnlineStat, f::Function, r::Int = 1000)\n\n\nCreate a double-or-nothing bootstrap using \nr\n replicates of \no\n for estimate \nf(o)\n\n\nExample:\n\n\nBernoulliBootstrap\n(\nMean\n(),\n \nmean\n,\n \n1000\n)\n\n\n\n\n\n\nBiasMatrix\n\n\nAdda bias/intercept term to a matrix on the fly without creating or copying data:\n\n\n\n\nBiasMatrix(rand(10,5))\n is roughly equivalent to \nhcat(rand(10,5), ones(10))\n\n\n\n\nBiasVector\n\n\nAdd a bias/intercept term to a vector on the fly without creating or copying data:\n\n\n\n\nBiasVector(rand(10))\n is roughly equivalent to \nvcat(rand(10), 1.0)\n\n\n\n\nBoundedEqualWeight\n\n\nOne of the \nWeight\n types.  Uses \nEqualWeight\n until reaching \n\u03bb = 2 / (1 + lookback)\n, then weights are held constant.\n\n\n\n\nBoundedEqualWeight(\u03bb::Float64)\n\n\nBoundedEqualWeight(lookback::Int)\n\n\n\n\nCovMatrix\n\n\nCovariance matrix, similar to \ncov(x)\n.\n\n\no\n \n=\n \nCovMatrix\n(\nx\n,\n \nEqualWeight\n())\n\n\no\n \n=\n \nCovMatrix\n(\nx\n)\n\n\nfit!\n(\no\n,\n \nx2\n)\n\n\n\ncor\n(\no\n)\n\n\ncov\n(\no\n)\n\n\nmean\n(\no\n)\n\n\nvar\n(\no\n)\n\n\n\n\n\n\nDiff\n\n\nTrack the last value and the last difference.\n\n\no\n \n=\n \nDiff\n()\n\n\no\n \n=\n \nDiff\n(\ny\n)\n\n\n\n\n\n\nDiffs\n\n\nTrack the last value and the last difference for multiple series.  Ignores \nWeight\n.\n\n\no\n \n=\n \nDiffs\n()\n\n\no\n \n=\n \nDiffs\n(\ny\n)\n\n\n\n\n\n\nElasticNetPenalty\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.ElasticNetPenalty \n: OnlineStats.Penalty\n\n\n\n\n\nFields:\n\n\n\u03bb :: Float64\na :: Float64\n\n\n\n\n\nEqualWeight\n\n\nOne of the \nWeight\n types.  Observations are weighted equally.  For analytical updates, the online algorithm will give results equal to the offline version.\n\n\n\n\nEqualWeight()\n\n\n\n\nExponentialWeight\n\n\nOne of the \nWeight\n types.  Updates are performed with a constant weight \n\u03bb = 2 / (1 + lookback)\n.\n\n\n\n\nExponentialWeight(\u03bb::Float64)\n\n\nExponentialWeight(lookback::Int)\n\n\n\n\nExtrema\n\n\nExtrema (maximum and minimum).\n\n\no\n \n=\n \nExtrema\n(\ny\n)\n\n\nfit!\n(\no\n,\n \ny2\n)\n\n\nextrema\n(\no\n)\n\n\n\n\n\n\nFitBeta\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.FitBeta{W\n:OnlineStats.Weight} \n: OnlineStats.DistributionStat{OnlineStats.ScalarInput}\n\n\n\n\n\nFields:\n\n\nvalue :: Distributions.Beta\nvar   :: OnlineStats.Variance{W\n:OnlineStats.Weight}\n\n\n\n\n\nFitCategorical\n\n\nFind the proportions for each unique input.  Categories are sorted by proportions. Ignores \nWeight\n.\n\n\no\n \n=\n \nFitCategorical\n(\ny\n)\n\n\n\n\n\n\nFitCauchy\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.FitCauchy{W\n:OnlineStats.Weight} \n: OnlineStats.DistributionStat{OnlineStats.ScalarInput}\n\n\n\n\n\nFields:\n\n\nvalue :: Distributions.Cauchy\nq     :: OnlineStats.QuantileMM{W\n:OnlineStats.Weight}\n\n\n\n\n\nFitGamma\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.FitGamma{W\n:OnlineStats.Weight} \n: OnlineStats.DistributionStat{OnlineStats.ScalarInput}\n\n\n\n\n\nFields:\n\n\nvalue :: Distributions.Gamma{T\n:Real}\nvar   :: OnlineStats.Variance{W\n:OnlineStats.Weight}\n\n\n\n\n\nFitLogNormal\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.FitLogNormal{W\n:OnlineStats.Weight} \n: OnlineStats.DistributionStat{OnlineStats.ScalarInput}\n\n\n\n\n\nFields:\n\n\nvalue :: Distributions.LogNormal\nvar   :: OnlineStats.Variance{W\n:OnlineStats.Weight}\n\n\n\n\n\nFitMultinomial\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.FitMultinomial{W\n:OnlineStats.Weight} \n: OnlineStats.DistributionStat{OnlineStats.VectorInput}\n\n\n\n\n\nFields:\n\n\nvalue :: Distributions.Multinomial\nmeans :: OnlineStats.Means{W\n:OnlineStats.Weight}\n\n\n\n\n\nFitMvNormal\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.FitMvNormal{W\n:OnlineStats.Weight} \n: OnlineStats.DistributionStat{OnlineStats.VectorInput}\n\n\n\n\n\nFields:\n\n\nvalue :: Distributions.MvNormal{T\n:Real,Cov\n:PDMats.AbstractPDMat,Mean\n:Union{Array{T,1},Distributions.ZeroVector}}\ncov   :: OnlineStats.CovMatrix{W\n:OnlineStats.Weight}\n\n\n\n\n\nFitNormal\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.FitNormal{W\n:OnlineStats.Weight} \n: OnlineStats.DistributionStat{OnlineStats.ScalarInput}\n\n\n\n\n\nFields:\n\n\nvalue :: Distributions.Normal{T\n:Real}\nvar   :: OnlineStats.Variance{W\n:OnlineStats.Weight}\n\n\n\n\n\nFrozenBootstrap\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.FrozenBootstrap \n: OnlineStats.Bootstrap{OnlineStats.ScalarInput}\n\n\n\n\n\nFields:\n\n\ncached_state :: Array{Float64,1}\nn            :: Int64\n\n\n\n\n\nHuberRegression\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.HuberRegression \n: OnlineStats.Model\n\n\n\n\n\nFields:\n\n\n\u03b4 :: Float64\n\n\n\n\n\nHyperLogLog\n\n\nHyperLogLog(b)\n\n\nApproximate count of distinct elements.  \nHyperLogLog\n differs from other OnlineStats in that any input to \nfit!(o::HyperLogLog, input)\n is considered a singleton.  Thus, a vector of inputs must be done by:\n\n\no\n \n=\n \nHyperLogLog\n(\n4\n)\n\n\nfor\n \nyi\n \nin\n \ny\n\n    \nfit!\n(\no\n,\n \nyi\n)\n\n\nend\n\n\n\n\n\n\nKMeans\n\n\nApproximate K-Means clustering of multivariate data.\n\n\no\n \n=\n \nKMeans\n(\ny\n,\n \n3\n,\n \nLearningRate\n())\n\n\nvalue\n(\no\n)\n\n\n\n\n\n\nL1Regression\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.L1Regression \n: OnlineStats.Model\n\n\n\n\n\nLassoPenalty\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.LassoPenalty \n: OnlineStats.Penalty\n\n\n\n\n\nFields:\n\n\n\u03bb :: Float64\n\n\n\n\n\nLearningRate\n\n\nOne of the \nWeight\n types.  It's primary use is for the OnlineStats that use stochastic approximation (\nStatLearn\n, \nQuantReg\n, \nQuantileMM\n, \nQuantileSGD\n, \nNormalMix\n, and \nKMeans\n).  The weight at update \nt\n is \n1 / t ^ r\n.  When weights reach \n\u03bb\n, they are held consant.  Compare to \nLearningRate2\n.\n\n\n\n\nLearningRate(r = 0.5, \u03bb = 0.0)\n\n\n\n\nLearningRate2\n\n\nOne of the \nWeight\n types.  It's primary use is for the OnlineStats that use stochastic approximation (\nStatLearn\n, \nQuantReg\n, \nQuantileMM\n, \nQuantileSGD\n, \nNormalMix\n, and \nKMeans\n).  The weight at update \nt\n is \n1 / (1 + c * (t - 1))\n.  When weights reach \n\u03bb\n, they are held consant.  Compare to \nLearningRate\n.\n\n\n\n\nLearningRate2(c = 0.5, \u03bb = 0.0)\n\n\n\n\nLinReg\n\n\nAnalytical Linear Regression.\n\n\nWith \nEqualWeight\n, this is equivalent to offline linear regression.\n\n\nusing OnlineStats, StatsBase\no = LinReg(x, y, wgt = EqualWeight())\ncoef(o)\ncoeftable(o)\nvcov(o)\nstderr(o)\npredict(o, x)\nconfint(o, .95)\n\n\n\n\n\nLinearRegression\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.LinearRegression \n: OnlineStats.Model\n\n\n\n\n\nLogRegMM\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.LogRegMM{W\n:OnlineStats.Weight} \n: OnlineStats.OnlineStat{OnlineStats.XYInput}\n\n\n\n\n\nFields:\n\n\n\u03b2      :: Array{Float64,1}\nH      :: Array{Float64,2}\nA      :: Array{Float64,2}\nb      :: Array{Float64,1}\n\u03b7      :: Float64\nweight :: W\n:OnlineStats.Weight\n\n\n\n\n\nLogisticRegression\n\n\nFor data in {0, 1}\n\n\nMean\n\n\nMean of a single series.\n\n\ny\n \n=\n \nrandn\n(\n100\n)\n\n\n\no\n \n=\n \nMean\n()\n\n\nfit!\n(\no\n,\n \ny\n)\n\n\n\no\n \n=\n \nMean\n(\ny\n)\n\n\n\n\n\n\nMeans\n\n\nMeans of multiple series, similar to \nmean(x, 1)\n.\n\n\nx\n \n=\n \nrandn\n(\n1000\n,\n \n5\n)\n\n\no\n \n=\n \nMeans\n(\n5\n)\n\n\nfit!\n(\no\n,\n \nx\n)\n\n\nmean\n(\no\n)\n\n\n\n\n\n\nModel\n\n\nNo documentation found.\n\n\nSummary:\n\n\nabstract OnlineStats.Model \n: Any\n\n\n\n\n\nSubtypes:\n\n\nOnlineStats.BivariateModel\nOnlineStats.HuberRegression\nOnlineStats.L1Regression\nOnlineStats.LinearRegression\nOnlineStats.PoissonRegression\nOnlineStats.QuantileRegression\n\n\n\n\n\nMoments\n\n\nUnivariate, first four moments.  Provides \nmean\n, \nvar\n, \nskewness\n, \nkurtosis\n\n\no\n \n=\n \nMoments\n(\nx\n,\n \nEqualWeight\n())\n\n\no\n \n=\n \nMoments\n(\nx\n)\n\n\nfit!\n(\no\n,\n \nx2\n)\n\n\n\nmean\n(\no\n)\n\n\nvar\n(\no\n)\n\n\nstd\n(\no\n)\n\n\nStatsBase\n.\nskewness\n(\no\n)\n\n\nStatsBase\n.\nkurtosis\n(\no\n)\n\n\n\n\n\n\nNoPenalty\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.NoPenalty \n: OnlineStats.Penalty\n\n\n\n\n\nNormalMix\n\n\nNormal Mixture of \nk\n components via an online EM algorithm.  \nstart\n is a keyword argument specifying the initial parameters.\n\n\no\n \n=\n \nNormalMix\n(\n2\n,\n \nLearningRate\n();\n \nstart\n \n=\n \nMixtureModel\n(\nNormal\n,\n \n[(\n0\n,\n \n1\n),\n \n(\n3\n,\n \n1\n)]))\n\n\nmean\n(\no\n)\n\n\nvar\n(\no\n)\n\n\nstd\n(\no\n)\n\n\n\n\n\n\nOnlineStat\n\n\nNo documentation found.\n\n\nSummary:\n\n\nabstract OnlineStats.OnlineStat{I\n:OnlineStats.Input} \n: Any\n\n\n\n\n\nSubtypes:\n\n\nOnlineStats.Bootstrap{I\n:OnlineStats.Input}\nOnlineStats.CovMatrix{W\n:OnlineStats.Weight}\nOnlineStats.Diffs{T\n:Real}\nOnlineStats.Diff{T\n:Real}\nOnlineStats.DistributionStat{I\n:OnlineStats.Input}\nOnlineStats.Extrema\nOnlineStats.HyperLogLog\nOnlineStats.KMeans{W\n:OnlineStats.Weight}\nOnlineStats.LinReg{W\n:OnlineStats.Weight}\nOnlineStats.LogRegMM{W\n:OnlineStats.Weight}\nOnlineStats.Means{W\n:OnlineStats.Weight}\nOnlineStats.Mean{W\n:OnlineStats.Weight}\nOnlineStats.Moments{W\n:OnlineStats.Weight}\nOnlineStats.QuantRegMM{W\n:OnlineStats.Weight}\nOnlineStats.QuantileMM{W\n:OnlineStats.Weight}\nOnlineStats.QuantileSGD{W\n:OnlineStats.StochasticWeight}\nOnlineStats.StatLearn{A\n:OnlineStats.Algorithm,M\n:OnlineStats.Model,P\n:OnlineStats.Penalty,W\n:OnlineStats.StochasticWeight}\nOnlineStats.Sums{T\n:Real}\nOnlineStats.Sum{T\n:Real}\nOnlineStats.Variances{W\n:OnlineStats.Weight}\nOnlineStats.Variance{W\n:OnlineStats.Weight}\n\n\n\n\n\nPenalty\n\n\nNo documentation found.\n\n\nSummary:\n\n\nabstract OnlineStats.Penalty \n: Any\n\n\n\n\n\nSubtypes:\n\n\nOnlineStats.ElasticNetPenalty\nOnlineStats.LassoPenalty\nOnlineStats.NoPenalty\nOnlineStats.RidgePenalty\n\n\n\n\n\nPoissonBootstrap\n\n\nNo documentation found.\n\n\nOnlineStats.PoissonBootstrap\n is a \nFunction\n.\n\n\n# 4 methods for generic function \nPoissonBootstrap\n:\nPoissonBootstrap{T\n:OnlineStats.ScalarInput}(o::OnlineStats.OnlineStat{T}, f::Function) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:63\nPoissonBootstrap{T\n:OnlineStats.ScalarInput}(o::OnlineStats.OnlineStat{T}, f::Function, r::Int64) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:63\nPoissonBootstrap{T\n:OnlineStats.VectorInput}(o::OnlineStats.OnlineStat{T}, f::Function) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:76\nPoissonBootstrap{T\n:OnlineStats.VectorInput}(o::OnlineStats.OnlineStat{T}, f::Function, r::Int64) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:76\n\n\n\n\n\nPoissonRegression\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.PoissonRegression \n: OnlineStats.Model\n\n\n\n\n\nQuantRegMM\n\n\nOnline MM Algorithm for Quantile Regression.\n\n\nQuantileMM\n\n\nApproximate quantiles via an online MM algorithm.  Typically more accurate than \nQuantileSGD\n.\n\n\no\n \n=\n \nQuantileMM\n(\ny\n,\n \nLearningRate\n())\n\n\no\n \n=\n \nQuantileMM\n(\ny\n,\n \ntau\n \n=\n \n[\n.\n25\n,\n \n.\n5\n,\n \n.\n75\n])\n\n\nfit!\n(\no\n,\n \ny2\n)\n\n\n\n\n\n\nQuantileRegression\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.QuantileRegression \n: OnlineStats.Model\n\n\n\n\n\nFields:\n\n\n\u03c4 :: Float64\n\n\n\n\n\nQuantileSGD\n\n\nApproximate quantiles via stochastic gradient descent.\n\n\no\n \n=\n \nQuantileSGD\n(\ny\n,\n \nLearningRate\n())\n\n\no\n \n=\n \nQuantileSGD\n(\ny\n,\n \ntau\n \n=\n \n[\n.\n25\n,\n \n.\n5\n,\n \n.\n75\n])\n\n\nfit!\n(\no\n,\n \ny2\n)\n\n\n\n\n\n\nRidgePenalty\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.RidgePenalty \n: OnlineStats.Penalty\n\n\n\n\n\nFields:\n\n\n\u03bb :: Float64\n\n\n\n\n\nSGD\n\n\nNo documentation found.\n\n\nSummary:\n\n\nimmutable OnlineStats.SGD \n: OnlineStats.Algorithm\n\n\n\n\n\nSVMLike\n\n\nFor data in {-1, 1}\n\n\nStatLearn\n\n\nNo documentation found.\n\n\nSummary:\n\n\ntype OnlineStats.StatLearn{A\n:OnlineStats.Algorithm,M\n:OnlineStats.Model,P\n:OnlineStats.Penalty,W\n:OnlineStats.StochasticWeight} \n: OnlineStats.OnlineStat{OnlineStats.XYInput}\n\n\n\n\n\nFields:\n\n\n\u03b20        :: Float64\n\u03b2         :: Array{Float64,1}\nintercept :: Bool\n\u03b7         :: Float64\nH0        :: Float64\nG0        :: Float64\nH         :: Array{Float64,1}\nG         :: Array{Float64,1}\nalgorithm :: A\n:OnlineStats.Algorithm\nmodel     :: M\n:OnlineStats.Model\npenalty   :: P\n:OnlineStats.Penalty\nweight    :: W\n:OnlineStats.StochasticWeight\n\n\n\n\n\nSum\n\n\nTrack the running sum.  Ignores \nWeight\n.\n\n\no\n \n=\n \nSum\n()\n\n\no\n \n=\n \nSum\n(\ny\n)\n\n\n\n\n\n\nSums\n\n\nTrack the running sum for multiple series.  Ignores \nWeight\n.\n\n\no\n \n=\n \nSums\n()\n\n\no\n \n=\n \nSums\n(\ny\n)\n\n\n\n\n\n\nTwoWayInteractionMatrix\n\n\nAdd second-order interaction terms on the fly without creating or copying data:\n\n\n\n\nTwoWayInteractionMatrix(rand(n, p))\n \"adds\" the \nbinomial(p, 2)\n interaction terms\n\n\n\n\nto each row\n\n\nTwoWayInteractionVector\n\n\nAdd second-order interaction terms on the fly without creating or copying data:\n\n\n\n\nTwoWayInteractionVector(rand(p))\n \"adds\" the \nbinomial(p, 2)\n interaction terms\n\n\n\n\nVariance\n\n\nUnivariate variance.\n\n\ny\n \n=\n \nrandn\n(\n100\n)\n\n\no\n \n=\n \nVariance\n(\ny\n)\n\n\nmean\n(\no\n)\n\n\nvar\n(\no\n)\n\n\nstd\n(\no\n)\n\n\n\n\n\n\nVariances\n\n\nVariances of a multiple series, similar to \nvar(x, 1)\n.\n\n\no\n \n=\n \nVariances\n(\nx\n,\n \nEqualWeight\n())\n\n\no\n \n=\n \nVariances\n(\nx\n)\n\n\nfit!\n(\no\n,\n \nx2\n)\n\n\n\nmean\n(\no\n)\n\n\nvar\n(\no\n)\n\n\nstd\n(\no\n)\n\n\n\n\n\n\nWeight\n\n\nNo documentation found.\n\n\nSummary:\n\n\nabstract OnlineStats.Weight \n: Any\n\n\n\n\n\nSubtypes:\n\n\nOnlineStats.BatchWeight\nOnlineStats.BoundedEqualWeight\nOnlineStats.ExponentialWeight\n\n\n\n\n\ncached_state\n\n\nNo documentation found.\n\n\nOnlineStats.cached_state\n is a \nFunction\n.\n\n\n# 3 methods for generic function \ncached_state\n:\ncached_state(b::OnlineStats.FrozenBootstrap) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:103\ncached_state(b::OnlineStats.Bootstrap{OnlineStats.ScalarInput}) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:116\ncached_state(b::OnlineStats.Bootstrap{OnlineStats.VectorInput}) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:125\n\n\n\n\n\ncenter\n\n\nNo documentation found.\n\n\nOnlineStats.center\n is a \nFunction\n.\n\n\n# 4 methods for generic function \ncenter\n:\ncenter(o::OnlineStats.Mean, x::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:24\ncenter{T\n:Real}(o::OnlineStats.Means, x::AbstractArray{T,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:49\ncenter(o::OnlineStats.Variance, x::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:80\ncenter{T\n:Real}(o::OnlineStats.Variances, x::AbstractArray{T,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:126\n\n\n\n\n\nclassify\n\n\nNo documentation found.\n\n\nOnlineStats.classify\n is a \nFunction\n.\n\n\n# 4 methods for generic function \nclassify\n:\n\n\nclassify\n(\nm::OnlineStats.BivariateModel, \u03b7::Array{T\n:Any,1}) at /Users/joshday/.\njulia\n/\nv0\n.5\n/\nOnlineStats\n/\nsrc\n/\nmodeling\n/\ntemp\n.\njl:45\n\n\nclassify\n(\nm::OnlineStats.LogisticRegression, \u03b7::Real) at /Users/joshday/.\njulia\n/\nv0\n.5\n/\nOnlineStats\n/\nsrc\n/\nmodeling\n/\ntemp\n.\njl:69\n\n\nclassify\n(\nm::OnlineStats.SVMLike, \u03b7::Real) at /Users/joshday/.\njulia\n/\nv0\n.5\n/\nOnlineStats\n/\nsrc\n/\nmodeling\n/\ntemp\n.\njl:99\n\n\nclassify\n(\no::OnlineStats\n.\nStatLearn\n, \nx\n) \nat\n /\nUsers\n/\njoshday\n/.\njulia\n/\nv0\n.5\n/\nOnlineStats\n/\nsrc\n/\nmodeling\n/\nstatlearn\n.\njl:81\n\n\n\n\n\n\ncoef\n\n\nNo documentation found.\n\n\nStatsBase.coef\n is a \nFunction\n.\n\n\n# 9 methods for generic function \ncoef\n:\ncoef(obj::StatsBase.StatisticalModel) at /Users/joshday/.julia/v0.5/StatsBase/src/statmodels.jl:5\ncoef(o::OnlineStats.StatLearn) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:79\ncoef(o::OnlineStats.LinReg) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:65\ncoef(o::OnlineStats.LogRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:44\ncoef(o::OnlineStats.QuantRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/quantregmm.jl:19\ncoef(o::OnlineStats.StatLearn) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:79\ncoef(o::OnlineStats.LinReg) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:65\ncoef(o::OnlineStats.LogRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:44\ncoef(o::OnlineStats.QuantRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/quantregmm.jl:19\n\n\n\n\n\ncost\n\n\nNo documentation found.\n\n\nOnlineStats.cost\n is a \nFunction\n.\n\n\n# 2 methods for generic function \ncost\n:\ncost(o::OnlineStats.StatLearn, x::AbstractArray{T\n:Any,1}, y::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:88\ncost(o::OnlineStats.StatLearn, x::AbstractArray{T\n:Any,2}, y::AbstractArray{T\n:Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:89\n\n\n\n\n\nfit\n\n\nNo documentation found.\n\n\nStatsBase.fit\n is a \nFunction\n.\n\n\n#\n \n17\n \nmethods\n \nfor\n \ngeneric\n \nfunction\n \nfit\n:\n\n\nfit\n(:\n:Type\n{\nDistributions\n.\nBinomial\n}\n,\n \ndata\n:\n:Tuple\n{\nInt64\n,\nAbstractArray\n}\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\nunivariate\n/\ndiscrete\n/\nbinomial\n.jl\n:191\n\n\nfit\n(:\n:Type\n{\nDistributions\n.\nBinomial\n}\n,\n \ndata\n:\n:Tuple\n{\nInt64\n,\nAbstractArray\n}\n,\n \nw\n:\n:AbstractArray\n{\nFloat64\n,\nN\n:\nAny\n}\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\nunivariate\n/\ndiscrete\n/\nbinomial\n.jl\n:192\n\n\nfit\n(:\n:Type\n{\nDistributions\n.\nCategorical\n}\n,\n \ndata\n:\n:Tuple\n{\nInt64\n,\nAbstractArray\n}\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\nunivariate\n/\ndiscrete\n/\ncategorical\n.jl\n:272\n\n\nfit\n(:\n:Type\n{\nDistributions\n.\nCategorical\n}\n,\n \ndata\n:\n:Tuple\n{\nInt64\n,\nAbstractArray\n}\n,\n \nw\n:\n:AbstractArray\n{\nFloat64\n,\nN\n:\nAny\n}\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\nunivariate\n/\ndiscrete\n/\ncategorical\n.jl\n:273\n\n\nfit\n{\nT\n:\nReal\n}\n(:\n:Type\n{\nDistributions\n.\nBeta\n}\n,\n \nx\n:\n:AbstractArray\n{\nT\n,\nN\n:\nAny\n}\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\nunivariate\n/\ncontinuous\n/\nbeta\n.jl\n:114\n\n\nfit\n{\nT\n:\nReal\n}\n(:\n:Type\n{\nDistributions\n.\nCauchy\n}\n,\n \nx\n:\n:AbstractArray\n{\nT\n,\nN\n:\nAny\n}\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\nunivariate\n/\ncontinuous\n/\ncauchy\n.jl\n:91\n\n\nfit\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nv\n:\n:AbstractArray\n{\nT\n:\nAny\n,\n1\n}\n;\n \nclosed\n,\n \nnbins\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:165\n\n\nfit\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nv\n:\n:AbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\n \nedg\n:\n:AbstractArray\n{\nT\n:\nAny\n,\n1\n}\n;\n \nclosed\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:163\n\n\nfit\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nv\n:\n:AbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\n \nwv\n:\n:StatsBase\n.WeightVec\n;\n \nclosed\n,\n \nnbins\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:170\n\n\nfit\n{\nW\n}\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nv\n:\n:AbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\n \nwv\n:\n:StatsBase\n.WeightVec\n{\nW\n,\nVec\n:\nAbstractArray\n{\nT\n:\nReal\n,\n1\n}\n}\n,\n \nedg\n:\n:AbstractArray\n{\nT\n:\nAny\n,\n1\n}\n;\n \nclosed\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:168\n\n\nfit\n{\nN\n}\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nvs\n:\n:Tuple\n{\nVararg\n{\nAbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\nN\n}}\n,\n \nedges\n:\n:Tuple\n{\nVararg\n{\nAbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\nN\n}}\n;\n \nclosed\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:202\n\n\nfit\n{\nN\n}\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nvs\n:\n:Tuple\n{\nVararg\n{\nAbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\nN\n}}\n;\n \nclosed\n,\n \nnbins\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:204\n\n\nfit\n{\nN\n}\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nvs\n:\n:Tuple\n{\nVararg\n{\nAbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\nN\n}}\n,\n \nwv\n:\n:StatsBase\n.WeightVec\n;\n \nclosed\n,\n \nnbins\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:209\n\n\nfit\n{\nN\n,\nW\n}\n(:\n:Type\n{\nStatsBase\n.\nHistogram\n}\n,\n \nvs\n:\n:Tuple\n{\nVararg\n{\nAbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\nN\n}}\n,\n \nwv\n:\n:StatsBase\n.WeightVec\n{\nW\n,\nVec\n:\nAbstractArray\n{\nT\n:\nReal\n,\n1\n}\n}\n,\n \nedges\n:\n:Tuple\n{\nVararg\n{\nAbstractArray\n{\nT\n:\nAny\n,\n1\n}\n,\nN\n}}\n;\n \nclosed\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nhist\n.jl\n:207\n\n\nfit\n(\nobj\n:\n:StatsBase\n.StatisticalModel\n,\n \ndata\n...)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nStatsBase\n/\nsrc\n/\nstatmodels\n.jl\n:46\n\n\nfit\n{\nD\n:\nDistributions\n.\nDistribution\n{\nF\n:\nDistributions\n.\nVariateForm\n,\nS\n:\nDistributions\n.\nValueSupport\n}\n}\n(\ndt\n:\n:Type\n{\nD\n}\n,\n \nx\n)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\ngenericfit\n.jl\n:14\n\n\nfit\n{\nD\n:\nDistributions\n.\nDistribution\n{\nF\n:\nDistributions\n.\nVariateForm\n,\nS\n:\nDistributions\n.\nValueSupport\n}\n}\n(\ndt\n:\n:Type\n{\nD\n}\n,\n \nargs\n...)\n \nat\n \n/\nUsers\n/\njoshday\n/\n.julia\n/\nv0\n.5\n/\nDistributions\n/\nsrc\n/\ngenericfit\n.jl\n:15\n\n\n\n\n\n\nfit!\n\n\nUpdate an OnlineStat with more data.  Additional arguments after the input data provide extra control over how the updates are done.\n\n\ny = randn(100)\no = Mean()\n\nfit!(o, y)      # standard usage\n\nfit!(o, y, 10)  # update in minibatches of size 10\n\nfit!(o, y, .1)  # update using weight .1 for each observation\n\nwts = rand(100)\nfit!(o, y, wts) # update observation i using wts[i]\n\n\n\n\n\nUpdate an OnlineStat with more data.  Additional arguments after the input data provide extra control over how the updates are done.\n\n\ny = randn(100)\no = Mean()\n\nfit!(o, y)      # standard usage\n\nfit!(o, y, 10)  # update in minibatches of size 10\n\nfit!(o, y, .1)  # update using weight .1 for each observation\n\nwts = rand(100)\nfit!(o, y, wts) # update observation i using wts[i]\n\n\n\n\n\nfitdistribution\n\n\nEstimate the parameters of a distribution.\n\n\nusing\n \nDistributions\n\n\n# Univariate distributions\n\n\no\n \n=\n \nfitdistribution\n(\nBeta\n,\n \ny\n)\n\n\no\n \n=\n \nfitdistribution\n(\nCategorical\n,\n \ny\n)\n  \n# ignores Weight\n\n\no\n \n=\n \nfitdistribution\n(\nCauchy\n,\n \ny\n)\n\n\no\n \n=\n \nfitdistribution\n(\nGamma\n,\n \ny\n)\n\n\no\n \n=\n \nfitdistribution\n(\nLogNormal\n,\n \ny\n)\n\n\no\n \n=\n \nfitdistribution\n(\nNormal\n,\n \ny\n)\n\n\nmean\n(\no\n)\n\n\nvar\n(\no\n)\n\n\nstd\n(\no\n)\n\n\nparams\n(\no\n)\n\n\n\n# Multivariate distributions\n\n\no\n \n=\n \nfitdistribution\n(\nMultinomial\n,\n \nx\n)\n\n\no\n \n=\n \nfitdistribution\n(\nMvNormal\n,\n \nx\n)\n\n\nmean\n(\no\n)\n\n\nvar\n(\no\n)\n\n\nstd\n(\no\n)\n\n\ncov\n(\no\n)\n\n\n\n\n\n\nkurtosis\n\n\nkurtosis(v, [wv::WeightVec], m=mean(v))\n\n\n\n\n\nCompute the excess kurtosis of a real-valued array \nv\n, optionally specifying a weighting vector \nwv\n and a center \nm\n.\n\n\nloss\n\n\nNo documentation found.\n\n\nOnlineStats.loss\n is a \nFunction\n.\n\n\n# 12 methods for generic function \nloss\n:\nloss(m::OnlineStats.Model, y::Array{T\n:Any,1}, \u03b7::Array{T\n:Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:25\nloss(m::OnlineStats.LinearRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:53\nloss(m::OnlineStats.L1Regression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:59\nloss(m::OnlineStats.LogisticRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:66\nloss(m::OnlineStats.PoissonRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:89\nloss(m::OnlineStats.SVMLike, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:96\nloss(m::OnlineStats.QuantileRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:104\nloss(m::OnlineStats.HuberRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:113\nloss(o::OnlineStats.StatLearn, x::AbstractArray{T\n:Any,1}, y::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:86\nloss(o::OnlineStats.StatLearn, x::AbstractArray{T\n:Any,2}, y::AbstractArray{T\n:Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:87\nloss(o::OnlineStats.LinReg, x, y) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:115\nloss(o::OnlineStats.LogRegMM, x, y) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:48\n\n\n\n\n\nmaprows\n\n\nPerform operations on data in blocks.\n\n\nmaprows(f::Function, b::Integer, data...)\n\n\nThis function iteratively feeds \ndata\n in blocks of \nb\n observations to the function \nf\n.  The most common usage is with \ndo\n blocks:\n\n\n# Example 1\n\n\ny\n \n=\n \nrandn\n(\n50\n)\n\n\no\n \n=\n \nVariance\n()\n\n\nmaprows\n(\n10\n,\n \ny\n)\n \ndo\n \nyi\n\n    \nfit!\n(\no\n,\n \nyi\n)\n\n    \nprintln\n(\nUpdated with another batch!\n)\n\n\nend\n\n\n\n\n\n\nnobs\n\n\nnobs(obj::StatisticalModel)\n\n\n\n\n\nReturns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.\n\n\npredict\n\n\nNo documentation found.\n\n\nStatsBase.predict\n is a \nFunction\n.\n\n\n# 25 methods for generic function \npredict\n:\npredict(m::OnlineStats.LinearRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:55\npredict(m::OnlineStats.L1Regression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:61\npredict(m::OnlineStats.LogisticRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:68\npredict(m::OnlineStats.PoissonRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:91\npredict(m::OnlineStats.SVMLike, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:98\npredict(m::OnlineStats.QuantileRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:108\npredict(m::OnlineStats.HuberRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:120\npredict(m::OnlineStats.LinearRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:55\npredict(m::OnlineStats.L1Regression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:61\npredict(m::OnlineStats.LogisticRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:68\npredict(m::OnlineStats.PoissonRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:91\npredict(m::OnlineStats.SVMLike, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:98\npredict(m::OnlineStats.QuantileRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:108\npredict(m::OnlineStats.HuberRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:120\npredict(obj::StatsBase.RegressionModel) at /Users/joshday/.julia/v0.5/StatsBase/src/statmodels.jl:153\npredict(m::OnlineStats.Model, \u03b7::Array{T\n:Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:35\npredict(o::OnlineStats.StatLearn, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:80\npredict(o::OnlineStats.LinReg, x::AbstractArray{T\n:Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:103\npredict(o::OnlineStats.LinReg, x::AbstractArray{T\n:Any,2}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:105\npredict(o::OnlineStats.LogRegMM, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:47\npredict(m::OnlineStats.Model, \u03b7::Array{T\n:Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:35\npredict(o::OnlineStats.StatLearn, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:80\npredict(o::OnlineStats.LinReg, x::AbstractArray{T\n:Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:103\npredict(o::OnlineStats.LinReg, x::AbstractArray{T\n:Any,2}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:105\npredict(o::OnlineStats.LogRegMM, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:47\n\n\n\n\n\nreplicates\n\n\nNo documentation found.\n\n\nOnlineStats.replicates\n is a \nFunction\n.\n\n\n# 1 method for generic function \nreplicates\n:\nreplicates(b::OnlineStats.Bootstrap) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:143\n\n\n\n\n\nskewness\n\n\nskewness(v, [wv::WeightVec], m=mean(v))\n\n\n\n\n\nCompute the standardized skewness of a real-valued array \nv\n, optionally specifying a weighting vector \nwv\n and a center \nm\n.\n\n\nsweep!\n\n\nsweep!(A, k, inv = false)\n, \nsweep!(A, k, v, inv = false)\n\n\nSymmetric sweep operator of the matrix \nA\n on element \nk\n.  \nA\n is overwritten. \ninv = true\n will perform the inverse sweep.  Only the upper triangle is read and swept.\n\n\nAn optional vector \nv\n can be provided to avoid memory allocation. This requires \nlength(v) == size(A, 1)\n.  Both \nA\n and \nv\n will be overwritten.\n\n\nx\n \n=\n \nrandn\n(\n100\n,\n \n10\n)\n\n\nxtx\n \n=\n \nx\nx\n\n\nsweep!\n(\nxtx\n,\n \n1\n)\n\n\nsweep!\n(\nxtx\n,\n \n1\n,\n \ntrue\n)\n\n\n\n\n\n\nvalue\n\n\nThe associated value of an OnlineStat.\n\n\no = Mean()\nvalue(o)", 
            "title": "API/Examples"
        }, 
        {
            "location": "/api/#api", 
            "text": "", 
            "title": "API"
        }, 
        {
            "location": "/api/#adam", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.ADAM  : OnlineStats.Algorithm  Fields:  m1 :: Float64\nm2 :: Float64", 
            "title": "ADAM"
        }, 
        {
            "location": "/api/#adadelta", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.AdaDelta  : OnlineStats.Algorithm  Fields:  \u03c1 :: Float64", 
            "title": "AdaDelta"
        }, 
        {
            "location": "/api/#adagrad", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.AdaGrad  : OnlineStats.Algorithm", 
            "title": "AdaGrad"
        }, 
        {
            "location": "/api/#adagrad2", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.AdaGrad2  : OnlineStats.Algorithm", 
            "title": "AdaGrad2"
        }, 
        {
            "location": "/api/#algorithm", 
            "text": "No documentation found.  Summary:  abstract OnlineStats.Algorithm  : Any  Subtypes:  OnlineStats.ADAM\nOnlineStats.AdaDelta\nOnlineStats.AdaGrad\nOnlineStats.AdaGrad2\nOnlineStats.Momentum\nOnlineStats.SGD", 
            "title": "Algorithm"
        }, 
        {
            "location": "/api/#bernoullibootstrap", 
            "text": "BernoulliBootstrap(o::OnlineStat, f::Function, r::Int = 1000)  Create a double-or-nothing bootstrap using  r  replicates of  o  for estimate  f(o)  Example:  BernoulliBootstrap ( Mean (),   mean ,   1000 )", 
            "title": "BernoulliBootstrap"
        }, 
        {
            "location": "/api/#biasmatrix", 
            "text": "Adda bias/intercept term to a matrix on the fly without creating or copying data:   BiasMatrix(rand(10,5))  is roughly equivalent to  hcat(rand(10,5), ones(10))", 
            "title": "BiasMatrix"
        }, 
        {
            "location": "/api/#biasvector", 
            "text": "Add a bias/intercept term to a vector on the fly without creating or copying data:   BiasVector(rand(10))  is roughly equivalent to  vcat(rand(10), 1.0)", 
            "title": "BiasVector"
        }, 
        {
            "location": "/api/#boundedequalweight", 
            "text": "One of the  Weight  types.  Uses  EqualWeight  until reaching  \u03bb = 2 / (1 + lookback) , then weights are held constant.   BoundedEqualWeight(\u03bb::Float64)  BoundedEqualWeight(lookback::Int)", 
            "title": "BoundedEqualWeight"
        }, 
        {
            "location": "/api/#covmatrix", 
            "text": "Covariance matrix, similar to  cov(x) .  o   =   CovMatrix ( x ,   EqualWeight ())  o   =   CovMatrix ( x )  fit! ( o ,   x2 )  cor ( o )  cov ( o )  mean ( o )  var ( o )", 
            "title": "CovMatrix"
        }, 
        {
            "location": "/api/#diff", 
            "text": "Track the last value and the last difference.  o   =   Diff ()  o   =   Diff ( y )", 
            "title": "Diff"
        }, 
        {
            "location": "/api/#diffs", 
            "text": "Track the last value and the last difference for multiple series.  Ignores  Weight .  o   =   Diffs ()  o   =   Diffs ( y )", 
            "title": "Diffs"
        }, 
        {
            "location": "/api/#elasticnetpenalty", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.ElasticNetPenalty  : OnlineStats.Penalty  Fields:  \u03bb :: Float64\na :: Float64", 
            "title": "ElasticNetPenalty"
        }, 
        {
            "location": "/api/#equalweight", 
            "text": "One of the  Weight  types.  Observations are weighted equally.  For analytical updates, the online algorithm will give results equal to the offline version.   EqualWeight()", 
            "title": "EqualWeight"
        }, 
        {
            "location": "/api/#exponentialweight", 
            "text": "One of the  Weight  types.  Updates are performed with a constant weight  \u03bb = 2 / (1 + lookback) .   ExponentialWeight(\u03bb::Float64)  ExponentialWeight(lookback::Int)", 
            "title": "ExponentialWeight"
        }, 
        {
            "location": "/api/#extrema", 
            "text": "Extrema (maximum and minimum).  o   =   Extrema ( y )  fit! ( o ,   y2 )  extrema ( o )", 
            "title": "Extrema"
        }, 
        {
            "location": "/api/#fitbeta", 
            "text": "No documentation found.  Summary:  type OnlineStats.FitBeta{W :OnlineStats.Weight}  : OnlineStats.DistributionStat{OnlineStats.ScalarInput}  Fields:  value :: Distributions.Beta\nvar   :: OnlineStats.Variance{W :OnlineStats.Weight}", 
            "title": "FitBeta"
        }, 
        {
            "location": "/api/#fitcategorical", 
            "text": "Find the proportions for each unique input.  Categories are sorted by proportions. Ignores  Weight .  o   =   FitCategorical ( y )", 
            "title": "FitCategorical"
        }, 
        {
            "location": "/api/#fitcauchy", 
            "text": "No documentation found.  Summary:  type OnlineStats.FitCauchy{W :OnlineStats.Weight}  : OnlineStats.DistributionStat{OnlineStats.ScalarInput}  Fields:  value :: Distributions.Cauchy\nq     :: OnlineStats.QuantileMM{W :OnlineStats.Weight}", 
            "title": "FitCauchy"
        }, 
        {
            "location": "/api/#fitgamma", 
            "text": "No documentation found.  Summary:  type OnlineStats.FitGamma{W :OnlineStats.Weight}  : OnlineStats.DistributionStat{OnlineStats.ScalarInput}  Fields:  value :: Distributions.Gamma{T :Real}\nvar   :: OnlineStats.Variance{W :OnlineStats.Weight}", 
            "title": "FitGamma"
        }, 
        {
            "location": "/api/#fitlognormal", 
            "text": "No documentation found.  Summary:  type OnlineStats.FitLogNormal{W :OnlineStats.Weight}  : OnlineStats.DistributionStat{OnlineStats.ScalarInput}  Fields:  value :: Distributions.LogNormal\nvar   :: OnlineStats.Variance{W :OnlineStats.Weight}", 
            "title": "FitLogNormal"
        }, 
        {
            "location": "/api/#fitmultinomial", 
            "text": "No documentation found.  Summary:  type OnlineStats.FitMultinomial{W :OnlineStats.Weight}  : OnlineStats.DistributionStat{OnlineStats.VectorInput}  Fields:  value :: Distributions.Multinomial\nmeans :: OnlineStats.Means{W :OnlineStats.Weight}", 
            "title": "FitMultinomial"
        }, 
        {
            "location": "/api/#fitmvnormal", 
            "text": "No documentation found.  Summary:  type OnlineStats.FitMvNormal{W :OnlineStats.Weight}  : OnlineStats.DistributionStat{OnlineStats.VectorInput}  Fields:  value :: Distributions.MvNormal{T :Real,Cov :PDMats.AbstractPDMat,Mean :Union{Array{T,1},Distributions.ZeroVector}}\ncov   :: OnlineStats.CovMatrix{W :OnlineStats.Weight}", 
            "title": "FitMvNormal"
        }, 
        {
            "location": "/api/#fitnormal", 
            "text": "No documentation found.  Summary:  type OnlineStats.FitNormal{W :OnlineStats.Weight}  : OnlineStats.DistributionStat{OnlineStats.ScalarInput}  Fields:  value :: Distributions.Normal{T :Real}\nvar   :: OnlineStats.Variance{W :OnlineStats.Weight}", 
            "title": "FitNormal"
        }, 
        {
            "location": "/api/#frozenbootstrap", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.FrozenBootstrap  : OnlineStats.Bootstrap{OnlineStats.ScalarInput}  Fields:  cached_state :: Array{Float64,1}\nn            :: Int64", 
            "title": "FrozenBootstrap"
        }, 
        {
            "location": "/api/#huberregression", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.HuberRegression  : OnlineStats.Model  Fields:  \u03b4 :: Float64", 
            "title": "HuberRegression"
        }, 
        {
            "location": "/api/#hyperloglog", 
            "text": "HyperLogLog(b)  Approximate count of distinct elements.   HyperLogLog  differs from other OnlineStats in that any input to  fit!(o::HyperLogLog, input)  is considered a singleton.  Thus, a vector of inputs must be done by:  o   =   HyperLogLog ( 4 )  for   yi   in   y \n     fit! ( o ,   yi )  end", 
            "title": "HyperLogLog"
        }, 
        {
            "location": "/api/#kmeans", 
            "text": "Approximate K-Means clustering of multivariate data.  o   =   KMeans ( y ,   3 ,   LearningRate ())  value ( o )", 
            "title": "KMeans"
        }, 
        {
            "location": "/api/#l1regression", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.L1Regression  : OnlineStats.Model", 
            "title": "L1Regression"
        }, 
        {
            "location": "/api/#lassopenalty", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.LassoPenalty  : OnlineStats.Penalty  Fields:  \u03bb :: Float64", 
            "title": "LassoPenalty"
        }, 
        {
            "location": "/api/#learningrate", 
            "text": "One of the  Weight  types.  It's primary use is for the OnlineStats that use stochastic approximation ( StatLearn ,  QuantReg ,  QuantileMM ,  QuantileSGD ,  NormalMix , and  KMeans ).  The weight at update  t  is  1 / t ^ r .  When weights reach  \u03bb , they are held consant.  Compare to  LearningRate2 .   LearningRate(r = 0.5, \u03bb = 0.0)", 
            "title": "LearningRate"
        }, 
        {
            "location": "/api/#learningrate2", 
            "text": "One of the  Weight  types.  It's primary use is for the OnlineStats that use stochastic approximation ( StatLearn ,  QuantReg ,  QuantileMM ,  QuantileSGD ,  NormalMix , and  KMeans ).  The weight at update  t  is  1 / (1 + c * (t - 1)) .  When weights reach  \u03bb , they are held consant.  Compare to  LearningRate .   LearningRate2(c = 0.5, \u03bb = 0.0)", 
            "title": "LearningRate2"
        }, 
        {
            "location": "/api/#linreg", 
            "text": "Analytical Linear Regression.  With  EqualWeight , this is equivalent to offline linear regression.  using OnlineStats, StatsBase\no = LinReg(x, y, wgt = EqualWeight())\ncoef(o)\ncoeftable(o)\nvcov(o)\nstderr(o)\npredict(o, x)\nconfint(o, .95)", 
            "title": "LinReg"
        }, 
        {
            "location": "/api/#linearregression", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.LinearRegression  : OnlineStats.Model", 
            "title": "LinearRegression"
        }, 
        {
            "location": "/api/#logregmm", 
            "text": "No documentation found.  Summary:  type OnlineStats.LogRegMM{W :OnlineStats.Weight}  : OnlineStats.OnlineStat{OnlineStats.XYInput}  Fields:  \u03b2      :: Array{Float64,1}\nH      :: Array{Float64,2}\nA      :: Array{Float64,2}\nb      :: Array{Float64,1}\n\u03b7      :: Float64\nweight :: W :OnlineStats.Weight", 
            "title": "LogRegMM"
        }, 
        {
            "location": "/api/#logisticregression", 
            "text": "For data in {0, 1}", 
            "title": "LogisticRegression"
        }, 
        {
            "location": "/api/#mean", 
            "text": "Mean of a single series.  y   =   randn ( 100 )  o   =   Mean ()  fit! ( o ,   y )  o   =   Mean ( y )", 
            "title": "Mean"
        }, 
        {
            "location": "/api/#means", 
            "text": "Means of multiple series, similar to  mean(x, 1) .  x   =   randn ( 1000 ,   5 )  o   =   Means ( 5 )  fit! ( o ,   x )  mean ( o )", 
            "title": "Means"
        }, 
        {
            "location": "/api/#model", 
            "text": "No documentation found.  Summary:  abstract OnlineStats.Model  : Any  Subtypes:  OnlineStats.BivariateModel\nOnlineStats.HuberRegression\nOnlineStats.L1Regression\nOnlineStats.LinearRegression\nOnlineStats.PoissonRegression\nOnlineStats.QuantileRegression", 
            "title": "Model"
        }, 
        {
            "location": "/api/#moments", 
            "text": "Univariate, first four moments.  Provides  mean ,  var ,  skewness ,  kurtosis  o   =   Moments ( x ,   EqualWeight ())  o   =   Moments ( x )  fit! ( o ,   x2 )  mean ( o )  var ( o )  std ( o )  StatsBase . skewness ( o )  StatsBase . kurtosis ( o )", 
            "title": "Moments"
        }, 
        {
            "location": "/api/#nopenalty", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.NoPenalty  : OnlineStats.Penalty", 
            "title": "NoPenalty"
        }, 
        {
            "location": "/api/#normalmix", 
            "text": "Normal Mixture of  k  components via an online EM algorithm.   start  is a keyword argument specifying the initial parameters.  o   =   NormalMix ( 2 ,   LearningRate ();   start   =   MixtureModel ( Normal ,   [( 0 ,   1 ),   ( 3 ,   1 )]))  mean ( o )  var ( o )  std ( o )", 
            "title": "NormalMix"
        }, 
        {
            "location": "/api/#onlinestat", 
            "text": "No documentation found.  Summary:  abstract OnlineStats.OnlineStat{I :OnlineStats.Input}  : Any  Subtypes:  OnlineStats.Bootstrap{I :OnlineStats.Input}\nOnlineStats.CovMatrix{W :OnlineStats.Weight}\nOnlineStats.Diffs{T :Real}\nOnlineStats.Diff{T :Real}\nOnlineStats.DistributionStat{I :OnlineStats.Input}\nOnlineStats.Extrema\nOnlineStats.HyperLogLog\nOnlineStats.KMeans{W :OnlineStats.Weight}\nOnlineStats.LinReg{W :OnlineStats.Weight}\nOnlineStats.LogRegMM{W :OnlineStats.Weight}\nOnlineStats.Means{W :OnlineStats.Weight}\nOnlineStats.Mean{W :OnlineStats.Weight}\nOnlineStats.Moments{W :OnlineStats.Weight}\nOnlineStats.QuantRegMM{W :OnlineStats.Weight}\nOnlineStats.QuantileMM{W :OnlineStats.Weight}\nOnlineStats.QuantileSGD{W :OnlineStats.StochasticWeight}\nOnlineStats.StatLearn{A :OnlineStats.Algorithm,M :OnlineStats.Model,P :OnlineStats.Penalty,W :OnlineStats.StochasticWeight}\nOnlineStats.Sums{T :Real}\nOnlineStats.Sum{T :Real}\nOnlineStats.Variances{W :OnlineStats.Weight}\nOnlineStats.Variance{W :OnlineStats.Weight}", 
            "title": "OnlineStat"
        }, 
        {
            "location": "/api/#penalty", 
            "text": "No documentation found.  Summary:  abstract OnlineStats.Penalty  : Any  Subtypes:  OnlineStats.ElasticNetPenalty\nOnlineStats.LassoPenalty\nOnlineStats.NoPenalty\nOnlineStats.RidgePenalty", 
            "title": "Penalty"
        }, 
        {
            "location": "/api/#poissonbootstrap", 
            "text": "No documentation found.  OnlineStats.PoissonBootstrap  is a  Function .  # 4 methods for generic function  PoissonBootstrap :\nPoissonBootstrap{T :OnlineStats.ScalarInput}(o::OnlineStats.OnlineStat{T}, f::Function) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:63\nPoissonBootstrap{T :OnlineStats.ScalarInput}(o::OnlineStats.OnlineStat{T}, f::Function, r::Int64) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:63\nPoissonBootstrap{T :OnlineStats.VectorInput}(o::OnlineStats.OnlineStat{T}, f::Function) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:76\nPoissonBootstrap{T :OnlineStats.VectorInput}(o::OnlineStats.OnlineStat{T}, f::Function, r::Int64) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:76", 
            "title": "PoissonBootstrap"
        }, 
        {
            "location": "/api/#poissonregression", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.PoissonRegression  : OnlineStats.Model", 
            "title": "PoissonRegression"
        }, 
        {
            "location": "/api/#quantregmm", 
            "text": "Online MM Algorithm for Quantile Regression.", 
            "title": "QuantRegMM"
        }, 
        {
            "location": "/api/#quantilemm", 
            "text": "Approximate quantiles via an online MM algorithm.  Typically more accurate than  QuantileSGD .  o   =   QuantileMM ( y ,   LearningRate ())  o   =   QuantileMM ( y ,   tau   =   [ . 25 ,   . 5 ,   . 75 ])  fit! ( o ,   y2 )", 
            "title": "QuantileMM"
        }, 
        {
            "location": "/api/#quantileregression", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.QuantileRegression  : OnlineStats.Model  Fields:  \u03c4 :: Float64", 
            "title": "QuantileRegression"
        }, 
        {
            "location": "/api/#quantilesgd", 
            "text": "Approximate quantiles via stochastic gradient descent.  o   =   QuantileSGD ( y ,   LearningRate ())  o   =   QuantileSGD ( y ,   tau   =   [ . 25 ,   . 5 ,   . 75 ])  fit! ( o ,   y2 )", 
            "title": "QuantileSGD"
        }, 
        {
            "location": "/api/#ridgepenalty", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.RidgePenalty  : OnlineStats.Penalty  Fields:  \u03bb :: Float64", 
            "title": "RidgePenalty"
        }, 
        {
            "location": "/api/#sgd", 
            "text": "No documentation found.  Summary:  immutable OnlineStats.SGD  : OnlineStats.Algorithm", 
            "title": "SGD"
        }, 
        {
            "location": "/api/#svmlike", 
            "text": "For data in {-1, 1}", 
            "title": "SVMLike"
        }, 
        {
            "location": "/api/#statlearn", 
            "text": "No documentation found.  Summary:  type OnlineStats.StatLearn{A :OnlineStats.Algorithm,M :OnlineStats.Model,P :OnlineStats.Penalty,W :OnlineStats.StochasticWeight}  : OnlineStats.OnlineStat{OnlineStats.XYInput}  Fields:  \u03b20        :: Float64\n\u03b2         :: Array{Float64,1}\nintercept :: Bool\n\u03b7         :: Float64\nH0        :: Float64\nG0        :: Float64\nH         :: Array{Float64,1}\nG         :: Array{Float64,1}\nalgorithm :: A :OnlineStats.Algorithm\nmodel     :: M :OnlineStats.Model\npenalty   :: P :OnlineStats.Penalty\nweight    :: W :OnlineStats.StochasticWeight", 
            "title": "StatLearn"
        }, 
        {
            "location": "/api/#sum", 
            "text": "Track the running sum.  Ignores  Weight .  o   =   Sum ()  o   =   Sum ( y )", 
            "title": "Sum"
        }, 
        {
            "location": "/api/#sums", 
            "text": "Track the running sum for multiple series.  Ignores  Weight .  o   =   Sums ()  o   =   Sums ( y )", 
            "title": "Sums"
        }, 
        {
            "location": "/api/#twowayinteractionmatrix", 
            "text": "Add second-order interaction terms on the fly without creating or copying data:   TwoWayInteractionMatrix(rand(n, p))  \"adds\" the  binomial(p, 2)  interaction terms   to each row", 
            "title": "TwoWayInteractionMatrix"
        }, 
        {
            "location": "/api/#twowayinteractionvector", 
            "text": "Add second-order interaction terms on the fly without creating or copying data:   TwoWayInteractionVector(rand(p))  \"adds\" the  binomial(p, 2)  interaction terms", 
            "title": "TwoWayInteractionVector"
        }, 
        {
            "location": "/api/#variance", 
            "text": "Univariate variance.  y   =   randn ( 100 )  o   =   Variance ( y )  mean ( o )  var ( o )  std ( o )", 
            "title": "Variance"
        }, 
        {
            "location": "/api/#variances", 
            "text": "Variances of a multiple series, similar to  var(x, 1) .  o   =   Variances ( x ,   EqualWeight ())  o   =   Variances ( x )  fit! ( o ,   x2 )  mean ( o )  var ( o )  std ( o )", 
            "title": "Variances"
        }, 
        {
            "location": "/api/#weight", 
            "text": "No documentation found.  Summary:  abstract OnlineStats.Weight  : Any  Subtypes:  OnlineStats.BatchWeight\nOnlineStats.BoundedEqualWeight\nOnlineStats.ExponentialWeight", 
            "title": "Weight"
        }, 
        {
            "location": "/api/#cached_state", 
            "text": "No documentation found.  OnlineStats.cached_state  is a  Function .  # 3 methods for generic function  cached_state :\ncached_state(b::OnlineStats.FrozenBootstrap) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:103\ncached_state(b::OnlineStats.Bootstrap{OnlineStats.ScalarInput}) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:116\ncached_state(b::OnlineStats.Bootstrap{OnlineStats.VectorInput}) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:125", 
            "title": "cached_state"
        }, 
        {
            "location": "/api/#center", 
            "text": "No documentation found.  OnlineStats.center  is a  Function .  # 4 methods for generic function  center :\ncenter(o::OnlineStats.Mean, x::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:24\ncenter{T :Real}(o::OnlineStats.Means, x::AbstractArray{T,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:49\ncenter(o::OnlineStats.Variance, x::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:80\ncenter{T :Real}(o::OnlineStats.Variances, x::AbstractArray{T,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/summary.jl:126", 
            "title": "center"
        }, 
        {
            "location": "/api/#classify", 
            "text": "No documentation found.  OnlineStats.classify  is a  Function .  # 4 methods for generic function  classify :  classify ( m::OnlineStats.BivariateModel, \u03b7::Array{T :Any,1}) at /Users/joshday/. julia / v0 .5 / OnlineStats / src / modeling / temp . jl:45  classify ( m::OnlineStats.LogisticRegression, \u03b7::Real) at /Users/joshday/. julia / v0 .5 / OnlineStats / src / modeling / temp . jl:69  classify ( m::OnlineStats.SVMLike, \u03b7::Real) at /Users/joshday/. julia / v0 .5 / OnlineStats / src / modeling / temp . jl:99  classify ( o::OnlineStats . StatLearn ,  x )  at  / Users / joshday /. julia / v0 .5 / OnlineStats / src / modeling / statlearn . jl:81", 
            "title": "classify"
        }, 
        {
            "location": "/api/#coef", 
            "text": "No documentation found.  StatsBase.coef  is a  Function .  # 9 methods for generic function  coef :\ncoef(obj::StatsBase.StatisticalModel) at /Users/joshday/.julia/v0.5/StatsBase/src/statmodels.jl:5\ncoef(o::OnlineStats.StatLearn) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:79\ncoef(o::OnlineStats.LinReg) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:65\ncoef(o::OnlineStats.LogRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:44\ncoef(o::OnlineStats.QuantRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/quantregmm.jl:19\ncoef(o::OnlineStats.StatLearn) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:79\ncoef(o::OnlineStats.LinReg) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:65\ncoef(o::OnlineStats.LogRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:44\ncoef(o::OnlineStats.QuantRegMM) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/quantregmm.jl:19", 
            "title": "coef"
        }, 
        {
            "location": "/api/#cost", 
            "text": "No documentation found.  OnlineStats.cost  is a  Function .  # 2 methods for generic function  cost :\ncost(o::OnlineStats.StatLearn, x::AbstractArray{T :Any,1}, y::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:88\ncost(o::OnlineStats.StatLearn, x::AbstractArray{T :Any,2}, y::AbstractArray{T :Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:89", 
            "title": "cost"
        }, 
        {
            "location": "/api/#fit", 
            "text": "No documentation found.  StatsBase.fit  is a  Function .  #   17   methods   for   generic   function   fit :  fit (: :Type { Distributions . Binomial } ,   data : :Tuple { Int64 , AbstractArray } )   at   / Users / joshday / .julia / v0 .5 / Distributions / src / univariate / discrete / binomial .jl :191  fit (: :Type { Distributions . Binomial } ,   data : :Tuple { Int64 , AbstractArray } ,   w : :AbstractArray { Float64 , N : Any } )   at   / Users / joshday / .julia / v0 .5 / Distributions / src / univariate / discrete / binomial .jl :192  fit (: :Type { Distributions . Categorical } ,   data : :Tuple { Int64 , AbstractArray } )   at   / Users / joshday / .julia / v0 .5 / Distributions / src / univariate / discrete / categorical .jl :272  fit (: :Type { Distributions . Categorical } ,   data : :Tuple { Int64 , AbstractArray } ,   w : :AbstractArray { Float64 , N : Any } )   at   / Users / joshday / .julia / v0 .5 / Distributions / src / univariate / discrete / categorical .jl :273  fit { T : Real } (: :Type { Distributions . Beta } ,   x : :AbstractArray { T , N : Any } )   at   / Users / joshday / .julia / v0 .5 / Distributions / src / univariate / continuous / beta .jl :114  fit { T : Real } (: :Type { Distributions . Cauchy } ,   x : :AbstractArray { T , N : Any } )   at   / Users / joshday / .julia / v0 .5 / Distributions / src / univariate / continuous / cauchy .jl :91  fit (: :Type { StatsBase . Histogram } ,   v : :AbstractArray { T : Any , 1 } ;   closed ,   nbins )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :165  fit (: :Type { StatsBase . Histogram } ,   v : :AbstractArray { T : Any , 1 } ,   edg : :AbstractArray { T : Any , 1 } ;   closed )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :163  fit (: :Type { StatsBase . Histogram } ,   v : :AbstractArray { T : Any , 1 } ,   wv : :StatsBase .WeightVec ;   closed ,   nbins )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :170  fit { W } (: :Type { StatsBase . Histogram } ,   v : :AbstractArray { T : Any , 1 } ,   wv : :StatsBase .WeightVec { W , Vec : AbstractArray { T : Real , 1 } } ,   edg : :AbstractArray { T : Any , 1 } ;   closed )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :168  fit { N } (: :Type { StatsBase . Histogram } ,   vs : :Tuple { Vararg { AbstractArray { T : Any , 1 } , N }} ,   edges : :Tuple { Vararg { AbstractArray { T : Any , 1 } , N }} ;   closed )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :202  fit { N } (: :Type { StatsBase . Histogram } ,   vs : :Tuple { Vararg { AbstractArray { T : Any , 1 } , N }} ;   closed ,   nbins )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :204  fit { N } (: :Type { StatsBase . Histogram } ,   vs : :Tuple { Vararg { AbstractArray { T : Any , 1 } , N }} ,   wv : :StatsBase .WeightVec ;   closed ,   nbins )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :209  fit { N , W } (: :Type { StatsBase . Histogram } ,   vs : :Tuple { Vararg { AbstractArray { T : Any , 1 } , N }} ,   wv : :StatsBase .WeightVec { W , Vec : AbstractArray { T : Real , 1 } } ,   edges : :Tuple { Vararg { AbstractArray { T : Any , 1 } , N }} ;   closed )   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / hist .jl :207  fit ( obj : :StatsBase .StatisticalModel ,   data ...)   at   / Users / joshday / .julia / v0 .5 / StatsBase / src / statmodels .jl :46  fit { D : Distributions . Distribution { F : Distributions . VariateForm , S : Distributions . ValueSupport } } ( dt : :Type { D } ,   x )   at   / Users / joshday / .julia / v0 .5 / Distributions / src / genericfit .jl :14  fit { D : Distributions . Distribution { F : Distributions . VariateForm , S : Distributions . ValueSupport } } ( dt : :Type { D } ,   args ...)   at   / Users / joshday / .julia / v0 .5 / Distributions / src / genericfit .jl :15", 
            "title": "fit"
        }, 
        {
            "location": "/api/#fit_1", 
            "text": "Update an OnlineStat with more data.  Additional arguments after the input data provide extra control over how the updates are done.  y = randn(100)\no = Mean()\n\nfit!(o, y)      # standard usage\n\nfit!(o, y, 10)  # update in minibatches of size 10\n\nfit!(o, y, .1)  # update using weight .1 for each observation\n\nwts = rand(100)\nfit!(o, y, wts) # update observation i using wts[i]  Update an OnlineStat with more data.  Additional arguments after the input data provide extra control over how the updates are done.  y = randn(100)\no = Mean()\n\nfit!(o, y)      # standard usage\n\nfit!(o, y, 10)  # update in minibatches of size 10\n\nfit!(o, y, .1)  # update using weight .1 for each observation\n\nwts = rand(100)\nfit!(o, y, wts) # update observation i using wts[i]", 
            "title": "fit!"
        }, 
        {
            "location": "/api/#fitdistribution", 
            "text": "Estimate the parameters of a distribution.  using   Distributions  # Univariate distributions  o   =   fitdistribution ( Beta ,   y )  o   =   fitdistribution ( Categorical ,   y )    # ignores Weight  o   =   fitdistribution ( Cauchy ,   y )  o   =   fitdistribution ( Gamma ,   y )  o   =   fitdistribution ( LogNormal ,   y )  o   =   fitdistribution ( Normal ,   y )  mean ( o )  var ( o )  std ( o )  params ( o )  # Multivariate distributions  o   =   fitdistribution ( Multinomial ,   x )  o   =   fitdistribution ( MvNormal ,   x )  mean ( o )  var ( o )  std ( o )  cov ( o )", 
            "title": "fitdistribution"
        }, 
        {
            "location": "/api/#kurtosis", 
            "text": "kurtosis(v, [wv::WeightVec], m=mean(v))  Compute the excess kurtosis of a real-valued array  v , optionally specifying a weighting vector  wv  and a center  m .", 
            "title": "kurtosis"
        }, 
        {
            "location": "/api/#loss", 
            "text": "No documentation found.  OnlineStats.loss  is a  Function .  # 12 methods for generic function  loss :\nloss(m::OnlineStats.Model, y::Array{T :Any,1}, \u03b7::Array{T :Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:25\nloss(m::OnlineStats.LinearRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:53\nloss(m::OnlineStats.L1Regression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:59\nloss(m::OnlineStats.LogisticRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:66\nloss(m::OnlineStats.PoissonRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:89\nloss(m::OnlineStats.SVMLike, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:96\nloss(m::OnlineStats.QuantileRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:104\nloss(m::OnlineStats.HuberRegression, y::Real, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:113\nloss(o::OnlineStats.StatLearn, x::AbstractArray{T :Any,1}, y::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:86\nloss(o::OnlineStats.StatLearn, x::AbstractArray{T :Any,2}, y::AbstractArray{T :Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:87\nloss(o::OnlineStats.LinReg, x, y) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:115\nloss(o::OnlineStats.LogRegMM, x, y) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:48", 
            "title": "loss"
        }, 
        {
            "location": "/api/#maprows", 
            "text": "Perform operations on data in blocks.  maprows(f::Function, b::Integer, data...)  This function iteratively feeds  data  in blocks of  b  observations to the function  f .  The most common usage is with  do  blocks:  # Example 1  y   =   randn ( 50 )  o   =   Variance ()  maprows ( 10 ,   y )   do   yi \n     fit! ( o ,   yi ) \n     println ( Updated with another batch! )  end", 
            "title": "maprows"
        }, 
        {
            "location": "/api/#nobs", 
            "text": "nobs(obj::StatisticalModel)  Returns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.", 
            "title": "nobs"
        }, 
        {
            "location": "/api/#predict", 
            "text": "No documentation found.  StatsBase.predict  is a  Function .  # 25 methods for generic function  predict :\npredict(m::OnlineStats.LinearRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:55\npredict(m::OnlineStats.L1Regression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:61\npredict(m::OnlineStats.LogisticRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:68\npredict(m::OnlineStats.PoissonRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:91\npredict(m::OnlineStats.SVMLike, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:98\npredict(m::OnlineStats.QuantileRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:108\npredict(m::OnlineStats.HuberRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:120\npredict(m::OnlineStats.LinearRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:55\npredict(m::OnlineStats.L1Regression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:61\npredict(m::OnlineStats.LogisticRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:68\npredict(m::OnlineStats.PoissonRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:91\npredict(m::OnlineStats.SVMLike, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:98\npredict(m::OnlineStats.QuantileRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:108\npredict(m::OnlineStats.HuberRegression, \u03b7::Real) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:120\npredict(obj::StatsBase.RegressionModel) at /Users/joshday/.julia/v0.5/StatsBase/src/statmodels.jl:153\npredict(m::OnlineStats.Model, \u03b7::Array{T :Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:35\npredict(o::OnlineStats.StatLearn, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:80\npredict(o::OnlineStats.LinReg, x::AbstractArray{T :Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:103\npredict(o::OnlineStats.LinReg, x::AbstractArray{T :Any,2}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:105\npredict(o::OnlineStats.LogRegMM, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:47\npredict(m::OnlineStats.Model, \u03b7::Array{T :Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/temp.jl:35\npredict(o::OnlineStats.StatLearn, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/statlearn.jl:80\npredict(o::OnlineStats.LinReg, x::AbstractArray{T :Any,1}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:103\npredict(o::OnlineStats.LinReg, x::AbstractArray{T :Any,2}) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/linreg.jl:105\npredict(o::OnlineStats.LogRegMM, x) at /Users/joshday/.julia/v0.5/OnlineStats/src/modeling/logregmm.jl:47", 
            "title": "predict"
        }, 
        {
            "location": "/api/#replicates", 
            "text": "No documentation found.  OnlineStats.replicates  is a  Function .  # 1 method for generic function  replicates :\nreplicates(b::OnlineStats.Bootstrap) at /Users/joshday/.julia/v0.5/OnlineStats/src/streamstats/bootstrap.jl:143", 
            "title": "replicates"
        }, 
        {
            "location": "/api/#skewness", 
            "text": "skewness(v, [wv::WeightVec], m=mean(v))  Compute the standardized skewness of a real-valued array  v , optionally specifying a weighting vector  wv  and a center  m .", 
            "title": "skewness"
        }, 
        {
            "location": "/api/#sweep", 
            "text": "sweep!(A, k, inv = false) ,  sweep!(A, k, v, inv = false)  Symmetric sweep operator of the matrix  A  on element  k .   A  is overwritten.  inv = true  will perform the inverse sweep.  Only the upper triangle is read and swept.  An optional vector  v  can be provided to avoid memory allocation. This requires  length(v) == size(A, 1) .  Both  A  and  v  will be overwritten.  x   =   randn ( 100 ,   10 )  xtx   =   x x  sweep! ( xtx ,   1 )  sweep! ( xtx ,   1 ,   true )", 
            "title": "sweep!"
        }, 
        {
            "location": "/api/#value", 
            "text": "The associated value of an OnlineStat.  o = Mean()\nvalue(o)", 
            "title": "value"
        }
    ]
}